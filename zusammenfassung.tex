\documentclass[10pt,a4paper]{scrartcl}

\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage{xcolor,cancel}

\usepackage{wrapfig}

\usepackage[margin=2.5cm]{geometry}

\renewcommand{\arraystretch}{1.2}

\newif\ifincludeExamples
%\includeExamplestrue % comment out to disable examples

\title{Wahrscheinlichkeitsrechnung und Statistik}
\author{Christoph H\"usler $\langle$chuesler@hsr.ch$\rangle$ } 

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\median}{median}

\begin{document}
\maketitle
\section{Kombinatorik}
\begin{description}
\item[Produktregel] Auswahl von $k$ aus $n$ Objekten, die einzelnen Elemente sind \emph{unabhängig} voneinander: $$n^k$$
\item[Permutation] Mögliche Anordnungen (Reihenfolge, Permutation) von $n$ Objekten $$n!$$
\item[Kombination] Auswahl von $k$ aus $n$ Objekten, wobei jedes Objekt nur einmal gewählt werden kann:
    $$\frac{n(n-1)\cdots(n-k+1)}{1 \cdot 2 \cdots k} = \frac{n!}{k!(n-k)!} = \binom{n}{k}$$
    Die erste dieser Formeln ist gut geeignet zur Umsetzung in Integer, da Divisionen immer aufgehen und Zwischenresultate ``klein'' bleiben.
\end{description}

Für kompliziertere Situationen, z.B. mit Nebenbedingungen, lässt sich oft Symmetrie ausnutzen.

\ifincludeExamples
\subsection{Erzeugende Funktion}
Schwierige kombinatorische Fragestellungen lassen sich oft in ein algebraisches oder analytisches Problem umformulieren, welches wir per Computer lösen können.

\subsubsection{Beispiel: Bildung eines Betrags aus 1 und 5 Fr. Münzen} 
\paragraph{Teilaufgabe 1} Es gibt 1 Variante, den Betrag mit 1 Fr. Münzen zu bilden.
Die erzeugende Funktion sieht wie folgt aus (geometrische Reihe): $$1 + 1x + 1x^2 + 1x^3 + 1x^4 + \dots = \frac{1}{1-x}$$ 

\paragraph{Teilaufgabe 2} Wenn der Betrag durch 5 teilbar ist, gibt es genau eine Art den Betrag mit 5 Fr. Münzen zu bilden, wenn nicht, keine.
$$1 + 0x + 0x^2 + \dots + 1x^5 + 0x^6 + \dots  = 1 + x^5 + x^{10} + x^{15} + \cdots = \frac{1}{1-x^5}$$

\paragraph{Interpretation}
Der Koeffizient $a$ von $ax^k$ sagt aus, auf wie viele Arten ein Betrag von k Fr.\ gebildet werden kann. Das Produkt der beiden Reihen kombiniert dann die 1Fr. und 5Fr. Varianten (analog erweiterbar für weitere Münzen):
$$\operatorname{taylor}\left(\frac{1}{(1-x)(1-x^5)}, 0\right) = 1 + x + x^2 + x^3 + x^4 + 2x^5 + 2x^6 + \dots + 3x^{10} + \dots$$

Äquivalent kann das Problem auch als Serie von Vektoren ausgedrückt werden, welche per Faltung (Convolution) kombiniert werden.
\fi

\section{Ereignisse und ihre Wahrscheinlichkeit}
\subsection{Ereignis}
Ein Ereignis ist immer verbunden mit einem (wiederholbaren) Experiment. Entscheidend ist der Versuchsausgang.

$$\Omega = \text{Menge der Elementar-Ereignisse} = \left\{ \text{alle möglichen Versuchsausgäng}e \right\}$$
Ein Ereignis ist eine Teilmenge von $\Omega$. Die Menge aller Ereignisse ist also die Menge aller Teilmengen von $\Omega$ (auch wenn viele davon unmöglich sind).

\ifincludeExamples
Beispiel: 7er-Würfel: $\Omega = \left\{ 1, 2, 3, 4, 5, 6, 7 \right\}$, 
2 6er-Würfel: $\Omega = \left[1, 2, 3, 4, 5, 6\right] \times \left[1, 2, 3, 4 ,5, 6\right]$\\ 
\fi

Es können auch kompliziertere Ereignisse $A \subset \Omega$ definiert werden.
\ifincludeExamples
$$G = \text{``gerade Zahl gewúrfelt''} = \{2, 4, 6\}$$
$$U = \text{``ungerade Zahl gewúrfelt''} = \{1, 3, 5, 7\}$$
$$P = \text{``Primzahl gewúrfelt''} = \{2, 3, 5, 7\}$$
\fi

\subsection{Wahrscheinlichkeit}
Wir brauchen eine ``Übersetzungstabelle'' von der Alltagssprache in die Mengensprache. 

\begin{center}
\begin{tabular}{rl}
Ereignis A eingetreten & Versuchsausgang $\omega \in A$ \\ 
Ereignis A ist unmöglich & $\forall \omega: \omega \notin A \ \Rightarrow \ A = \varnothing \ \text{(das unmögliche Ereignis)}$  \\ 
Ereignis A trifft sicher ein & $\forall \omega: \omega \in A \ \Rightarrow \ A = \Omega \ \text{(das sichere Ereignis)}$ \\ 
A und B & $A \cap B$ \\ 
A oder B & $A \cup B$ \\ 
nicht A & $\Omega \setminus A$ \\ 
wenn A dann B & $A \subset B$  \\ 
A unter der Bedingung B & $A\ |\ B$ \\
\end{tabular}
\end{center}

Die Wahrscheinlichkeit eines Ereignisses $A$ wird geschrieben als $P(A)$.

$$P(A) = \lim_{\text{\footnotesize\#Versuche} \rightarrow \infty} \frac{\text{Anzahl Eintreten von A}}{\text{Anzahl Versuche}}$$

\ifincludeExamples
Beispiel fairer (7er-)Würfel: $$P(\{7\}) = \frac{1}{7},\ P(G) = \frac{3}{7}, \ P(\varnothing) = 0, \ P(\Omega) = 1$$
\fi

Die gemessenen Werte konvergieren mit einem Fehler der Grössenordnung $\frac{1}{\sqrt{n}}$ (eine Nachkommastelle mehr entspricht $10^2$ mal mehr Experimenten).

\subsubsection{Formeln für Wahrscheinlichkeiten} 

\begin{center}
\begin{tabular}{rl}
$A \subset B$ & $P(A) \leq P(B)$ \\
$\varnothing \subset A \subset \Omega$ & $0 = P(\varnothing) \leq P(A) \leq P(\Omega) = 1$ \\
$\bar{A}$ & $P(\bar{A}) + P(A) = 1 \Rightarrow P(\bar{A}) = P(\Omega \setminus A) = 1 - P(A)$ \\
$A \cup B$ & $ P(A\cup B) = P(A) + P(B) - P(A\cap B)$ \\
A, B unabhängig & $P(A\cap B) \stackrel{!}{=} P(A)P(B)$\\[2pt]
$A\ |\ B$ & $P(A\ |\ B) = \frac{P(A\cap B)}{P(B)}$ \\[2pt]
$A$ wenn $A\ |\ B_i$ vorliegen & $ P(A) = \sum_{i=1}^n P(A\ |\ B_i) \cdot P(B_i) $
\end{tabular}
\end{center}

Intuition für diese Regeln ist die ``Flächenmessung'' (manchmal tatsächlich der Fall, z.B. Dartspiel).

\subsubsection{Satz von Bayes}
\begin{align*}
\text{Aus}\quad P(A \cap B) = P(B|A) & \cdot P(A) = P(A|B) \cdot P(B) \quad\text{folgt}\\
  P(A|B) = P(B|A) \cdot \frac{P(A)}{P(B)}\quad & \text{und} \quad P(B|A) = P(A|B) \cdot \frac{P(B)}{P(A)}
\end{align*}

\ifincludeExamples
\subsection{DNA-Test vor Gericht}
Mögliche Ereignisse:
\begin{itemize}
\item $D$ = DNA am Tatort entspricht DNA des Angeklagten
\item $T$ = Test zeigt Übereinstimmung
\end{itemize}

\begin{description} % use text for text in math environment ($ for inline resp. $$) 
\item[Anklage] $P(D|T) \text{ gross, also ist der Angeklagte der Täter}$
\item[Gericht] $P(T|D) = \text{Zuverlässigkeit des Tests}$
\item[Firma] $P(D|T) = \text{Testet, ob DNA-Test bei Übereinstimmung Ja sagt}$
\end{description}

Im Gerichtssaal: $P(D|T) = P(T|D) \cdot \frac{P(D)}{P(T)}$

Für ein wasserdichtes Alibi gilt: $P(D) = 0 \rightarrow P(D|T) = 0$ (Wasserdichtes Alibi schlägt DNA-Test).

Je grösser $P(T)$, desto kleiner $P(D|T)$, also die Wahrscheinlichkeit, dass man von diesem Test auf den Täter schliessen kann.

\subsection{HIV-Test}
Mögliche Ereignisse:
\begin{itemize}
\item $H$ = Kandidat hat Krankheit
\item $T$ = Test zeigt HIV an
\end{itemize}

Wir sind an $P(H|T)$ interessiert.

Wir `wissen':
\begin{itemize}
\item $P(H) = 0.0001$
\item $P(T|H) = 0.999$ Test zeigt HIV positiv bei einer infizierten Person an
\item $P(\bar{T}|\bar{H}) = 0.9999$ Test zeigt HIV negativ bei einer nicht infizierten Person an
\end{itemize}

\begin{align*} % & in each line will be aligned
P(H|T) &= P(T|H) \cdot \frac{P(H)}{P(T)} \quad\text{aber wir haben } P(T) \text{ nicht} \\
P(T) &= P(T|H) \cdot P(H) + P(T|\bar{H}) \cdot P(\bar{H}) \\
	 &= 0.999 \cdot 0.0001 + (1-0.9999) \cdot (1-0.0001) \\
	 &= 0.00019989 \approx 0.0002 \\
\Rightarrow P(H|T) &= 0.999 \cdot \frac{0.0001}{0.0002} \approx 0.5
\end{align*}

Umgekehrt:
$$P(\bar{H}|\bar{T}) = P(\bar{T}|\bar{H}) \cdot \frac{P(\bar{H})}{P(\bar{T})}
 = 0.9999 \cdot \frac{0.9999}{0.9998} \approx 1$$

\subsection{Karies bei Kindern und Schokoladenkonsum}
%Prüfungsaufgabe
Mögliche Ereignisse
\begin{itemize}
\item $Z$ = {schlechte Zähne}
\item $S$ = {Schoggikonsum}
\end{itemize}

Wir wissen:
$$P(Z) = 0.06 \qquad P(S|Z) = \frac{2}{3} \qquad P(S|\bar{Z}) = 0.17$$
\begin{align*}
P(Z|S) & = P(S|Z) \cdot \frac{P(Z)}{P(S)} \\
 P(S) & = P(S|Z) \cdot P(Z) + P(S|\bar{Z}) \cdot P(\bar{Z}) \\
 & = \frac{2}{3} \cdot 0.06 + 0.17 (1-0.06) \\
 & = 0.1998 \approx 20\% \\
 P(Z|S) &= \frac{2}{3} \cdot \frac{0.06}{0.2} = 20\%
\end{align*}

\subsection{Ziegen und Autos (Monty Hall-Problem)}
\begin{itemize}
\item 3 Türen: 1 Auto, 2 Ziegen
\item 1 von 3 Türen wählen
\item 2 Strategien: Bleiben, Wechseln
\end{itemize}

Ereignisse:
\begin{itemize}
\item $G = \text{Auto gewonnen}$
\item $A = \text{erste Wahl war ein Auto}$
\item $\bar{A} = \text{erste Wahl war eine Ziege}$
\end{itemize}

Finde Gewinnwahrscheinlichkeit
$$P(G) = P(G|A) \cdot P(A) + P(G|\bar{A}) \cdot P(\bar{A})$$
\begin{description}
\item[Strategie \emph{Bleiben}] $1 \cdot \frac{1}{3} + 0 \cdot \frac{2}{3} = \frac{1}{3}$
\item[Strategie \emph{Wechseln}] $0 \cdot \frac{1}{3} + 1 \cdot \frac{2}{3} = \frac{2}{3}$
\end{description}
\emph{Wechseln} ist also doppelt so gut wie \emph{Bleiben}.

\subsubsection{Strategie mit Münzwurf festgelegt}
\begin{itemize}
\item $B$: Bleibstrategie wurde gewählt
\item $W$: Wechselstrategie wurde gewählt
\end{itemize}
\begin{align*}
P(G) & = P(G|B) \cdot P(B) + P(G|W) \cdot P(W) \\
	& = \frac{1}{3}\cdot\frac{1}{2} + \frac{2}{3}\cdot\frac{1}{2} = \frac{1}{2}
\end{align*}
Optimale Strategie: Immer wechseln.

\subsection{Das Erfolgsgeheimnis von Google}
\begin{description}
\item[Problem] Vorkommen von Wörtern sagt nichts (mehr) über Relevanz aus.
\item[Idee von Google] Relevanz aus Internet $\rightarrow$ Pagerank. Finde Seite mit höchster Relevanz aus Linkstruktur.
\end{description}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{images/google-internet.png}
    \caption{Beispiel-Internet mit 8 Knoten}
    \label{fig:awesome_image}
\end{figure}

Ereignisse:
\begin{itemize}
\item $S_{i}$ = Surfer liest Seite $i$ ($P(S_i)$ misst Beliebtheit der Seite $S_i$)
\item $S_{j}'$ = Surfer liest Seite $j$ nachdem er geklickt hat.
\end{itemize}

Aus dem Satz der Totalen Wahrscheinlichtkeit folgt
$$P(S_j') = \sum_{i=1}^N P(S_j' | S_i) \cdot P(S_i)$$

Da die Anzahl Besucher pro Webseite vor und nach einem Klick ungefähr gleich bleibt können wir $P(S_j') = P(S_j)$ gleichsetzen. Dies ergibt ein Gleichungssystem (eine Gleichung pro Webseite).

$$H = 
\begin{matrix}
  S_1 & S_2 & S_3 & S_4 & S_5 & S_6 & S_7 & S_8 &  \\
  0 & 0 & 0 & 0 & 0 & 0 & \frac{1}{3} & 0 & S_1' \\
  \frac{1}{2} & 0 & \frac{1}{2} & \frac{1}{3} & 0 & 0 & 0 & 0 & S_2' \\
  \frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & S_3' \\
  0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & S_4' \\
  0 & 0 & \frac{1}{2} & \frac{1}{3} & 0 & 0 & \frac{1}{3} & 0 & S_5' \\
  0 & 0 & 0 & \frac{1}{3} & \frac{1}{2} & 0 & 0 & \frac{1}{2} & S_6' \\
  0 & 0 & 0 & 0 & \frac{1}{2} & 0 & 0 & \frac{1}{2} & S_7' \\
  0 & 0 & 0 & 0 & 0 & 1 & \frac{1}{3} & 0 & S_8' 
\end{matrix}$$


\begin{align*}
P(S_{j}') &= \sum\limits{i=1}^N P(S_{j}'|S_{i}) \cdot P(S_{i}) \\
P(S_{j}'|S_{i}) &= \text{Zeile von } H \\
\text{mit } p = \begin{pmatrix}
\dots \\
P(S_{j}) \\
\dots
\end{pmatrix},\quad p' &= Hp
\end{align*}

Mit der Zeit stabilisieren sich die Wahrscheinlichkeiten $P(S_i)$ und $p$ konvergiert zum Eigenvektor mit Eigenwert 1 von $H$.

\subsubsection{Berechnung des Eigenwertes}
$H$ ist eine sehr spezielle Matrix. Alle Spalten summieren zu $1$ auf, und alle anderen Eigenwerte sind kleiner als $1$.

\begin{align*}
p &= a_pp + a_1v_1 + a_2v_2 + \dots \\
H^np &= a_pp + a_1\lambda_1^nv_1 + a_2\lambda_2^nv_2 + \dots \\
\Rightarrow H^np &=a_pp
\end{align*}

Da die $\lambda_i$ alle kleiner als 1 sind verschwinden diese Terme mit grösseren $n$, und übrig bleibt $p$

\subsubsection{Freier Wille}
F: Surfer setzt freien Wille ein. $P(F) = 1 - \alpha$ (Konfigurationsparameter).

\begin{align*}
P(S_{j}') & = P(S_{j}'|F)\cdot P(F) + P(S_{j}'|\bar{F})\cdot P(\bar{F}) \\
\Rightarrow p & = \frac{1}{N}\cdot (1-\alpha) +  \alpha Hp
\end{align*}

G ist die Google-Matrix, A ist eine Matrix voller Einsen.
$$
p = Gp = \alpha Hp + (1-\alpha)\frac{1}{N}Ap \rightarrow G = \alpha H + (1-\alpha)\frac{1}{N}A 
$$
\fi

\section{Zufallsvariablen, Erwartungswert, Varianz}

\subsection{Zufallsvariablen}

Ereignisse treffen ein oder nicht. Wir wollen Versuchsausgängen aber einen Wert zuweisen können.
Beim Würfeln haben wir die unterschiedlichen Bilder auf den Würfel-Seiten als Wert interpretiert, d.h. wir haben implizit eine Funktion 
$$X: \Omega \rightarrow \mathbb{R}\ :\ \omega \rightarrow X(\omega)$$

$X$ ist eine Zufallsvariable (ZV). Diese Zufallsvariable definiert nun neue Ereignisse:
\begin{align*}
\{ X = x \} \quad& P(X = x) \\
\{ X \le a \} \quad& P(X\le a) \\
\{ X > a \} \quad& P(X > a)
\end{align*}

\subsection{Erwartungswert}
$X$ ist eine Zufallsvariable.

$$E(X) = \sum_{\text{Werte}} \text{Wert} \cdot \text{Wahrscheinlichkeit} = \sum_{i=1}^n x_i \cdot P(X = x_i) $$

\ifincludeExamples
\begin{center}
\begin{tabular}{c | c | c} 
Werte & Wahrscheinlichkeit \\ \hline
1 & $\frac{1}{6}$ & $1 \cdot \frac{1}{6}$ \\
2 & $\frac{1}{6}$ & $2 \cdot \frac{1}{6}$ \\
3 & $\frac{1}{6}$ & $3 \cdot \frac{1}{6}$ \\
4 & $\frac{1}{6}$ & $4 \cdot \frac{1}{6}$ \\
5 & $\frac{1}{6}$ & $5 \cdot \frac{1}{6}$ \\
6 & $\frac{1}{6}$ & $6 \cdot \frac{1}{6}$ \\ \cline{3-3}
\multicolumn{2}{c|}{} & $\frac{21}{6} = 3.5$  
\end{tabular}
\end{center} 

Intuition: Erwartungswert $E(X)$ verhält sich wie ein Integral $\int f(x) dx$.
\fi

\subsubsection{Rechenregeln für Erwartungswerte $E(\ .\ )$}

$E(\ .\ )$ ist eine lineare Funktion.
\begin{align*}
\text{Add. Linearität: } & E(X+Y) = E(X) + E(Y) \\
\text{Mult. Linearität: } & E(\lambda X) = \sum_{i=1}^n \lambda x_i P(X = x_i) = \lambda \sum_{i=1}^n x_i P(X = x_i) \\
X \text{ und } Y \text{ unabhängig } &\Longleftrightarrow \forall x,y: \{ X = x \} \text{ und } \{Y = y\} \implies E(XY) = E(X)E(Y)
\end{align*}

\subsection{Varianz}
Varianz ist die Abweichung nach ``links'' und ``rechts''. Intuitiv: $X - E(X)$. Aber:
$$ E(X - E(X)) = E(X) - E(E(X)) = E(X) - E(X) = 0 $$
Die negativen Vorzeichen stellen ein Problem dar. Da Beträge aus analytischer Sicht suboptimal sind (Nullpunkt) ist die Lösung die Quadratfunktion.

\begin{align*}
 \text{Varianz } \var(X) &= E((X - E(X))^2)  \\
 \text{Einfachere Formel: } \var(X) &= E(X^2 - 2XE(X) + E(X)^2) \\
& = E(X^2) - 2E(X)E(X) + E(X)^2 \\
& = E(X^2) - E(X)^2
\end{align*}

\ifincludeExamples
\subsubsection{Beispiel}
\begin{center}
\begin{tabular}{c | c | c | c | c}
X & P & $X^2$     & $X \cdot P(X)$ & $X^2 \cdot P(X)$ \\ \hline
1 & $\frac{1}{6}$ & 1  & $\frac{1}{6}$ & $\frac{1}{6}$ \\
2 & $\frac{1}{6}$ & 4  & $\frac{2}{6}$ & $\frac{4}{6}$ \\
3 & $\frac{1}{6}$ & 9  & $\frac{3}{6}$ & $\frac{9}{6}$ \\
4 & $\frac{1}{6}$ & 16 & $\frac{4}{6}$ & $\frac{16}{6}$ \\
5 & $\frac{1}{6}$ & 25 & $\frac{5}{6}$ & $\frac{25}{6}$ \\
6 & $\frac{1}{6}$ & 36 & $\frac{6}{6}$ & $\frac{36}{6}$ 
\end{tabular}
\end{center}

\begin{align*}
E(X) &= \frac{21}{6} = \frac{7}{2}, \quad E(X^2) = \frac{91}{6} \\
\var(X) & = E(X^2) - E(X)^2 = \frac{91}{6} - \frac{49}{4} = \frac{35}{12} \\
\sqrt{\var(X)} &\approx 1.707 \\
\end{align*}
\fi

\subsubsection{Rechenregeln für Varianz}
\begin{align*}
\var(\lambda X) & = E((\lambda X)^2) - E(\lambda X)^2  = \lambda^2 E(X^2) - \lambda^2E(X)^2 \\
               & = \lambda^2 \var(X) \\
\var(X+Y)       & = E((X+Y)^2) - E(X+Y)^2 \\
               & = E(X^2) + 2E(XY) + E(Y^2) - E(X^2) - 2E(X)E(Y) - E(Y^2) \\
               & = \var(X) + \var(Y) + 2(\underbrace{E(XY) - E(X)E(Y)}_{\cov(X, Y)}) 
\end{align*}

$\cov$ ist die Kovarianz, welche ist 0 wenn $X$ und $Y$ unabhängig sind.

\subsection{Genauigkeit des Mittelwertes}
$X$: Zufallsvariable, $\mu = E(X)$, $\,\varepsilon$: erwünschte Genauigkeit. \\
Gesucht: $ P(|X - \mu| > \,\varepsilon) $
 
\begin{itemize}
\item $\,\varepsilon \text{ klein } \Rightarrow P (|X-\mu| > \,\varepsilon) \text{ gross}$.
\item $\var(X) \text{ grösser } \Rightarrow P (|X-\mu| > \,\varepsilon) \text{ grösser}$.
\end{itemize}

\subsubsection{Satz von Tschebyscheff}
 $$P (|X-\mu| > \,\varepsilon) \le \frac{\var(X)}{\,\varepsilon^2}$$
\ifincludeExamples
\subsubsection{Beweis}
\begin{align*}
\text{Ereignis } A &= \{ |X-\mu| > \,\varepsilon \} \\
\text{besondere ZV: } \chi_A & = \begin{cases}1 & \quad A \text{ eingetreten} \\ 
                                              0 & \quad \text{sonst}
                                 \end{cases} & \text{logischerweise: } \chi^2 = \chi\\
E(\chi_A) & = 0\cdot P(\chi_A = 0) + 1\cdot P(\chi_A = 1) = P(A) \\
\varepsilon &\le |X - \mu| & \text{gilt falls } A \text{ eingetreten} \\
\varepsilon \chi_A &\le |X-\mu| & ()^2 \text{ statt Betrag} \\
\varepsilon^2\chi_A^{\cancel{2}} &\le (X-\mu)^2 & |\ E(\ .\ ) \\
\varepsilon^2P(A) = \,\varepsilon^2E(\chi_A)&\le E((X-E(X))^2) = \var(X) \\
\varepsilon^2P(|X-\mu|>\,\varepsilon) & \le \var(X)
\end{align*}
\fi

\subsubsection{Genauigkeit erhöhen}
Viele Messungen $X_1, X_2, \dots, X_n$. $ \text{Mittelwert } M_n = \frac{X_1+ X_2+ \dots + X_n}{n}$ wird \emph{genauer} mit mehr Messungen.

\begin{align*}
E(M_n) & = E\left( \frac{X_1+ X_2+ \dots + X_n}{n}\right)  \\
& = \frac{E(X_1) + E(X_2) + \dots + E(X_n)}{n} \\
& = E(X) \\
\var(M_n) &= var\left(\frac{X_1+ X_2+ \dots + X_n}{n}\right) \\
&\stackrel{\text{falls }X_i\text{ unabhängig}}{=} \frac{\var(X_1)+ \var(X_2)+ \dots + \var(X_n)}{n^2} \\ % FIXME text should be above =
& = \frac{n\cdot \var(X)}{n^2} \\
& = \frac{\var(X)}{n}
\end{align*}
Die Varianz wird also kleiner bei mehr \emph{unabhängigen} Messungen.

$$ P(|M_n - \mu| > \,\varepsilon) \le \frac{\var(M_n)}{\,\varepsilon^2} = \frac{\var(X)}{n\,\varepsilon^2} $$
Erwartungswert und Varianz lassen sich nun wie folgt ausdrücken:
\begin{align*}
E(X) & = \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n x_i \\
\var(X) & = \frac{1}{n} \sum_{i=1}^n x_i^2 - \left(\frac{1}{n} \sum_{i=1}^n x_i\right)^2
\end{align*}
Je mehr Messungen vorhanden sind, desto weniger wahrscheinlich ist eine Abweichung grösser als $\,\varepsilon$ (\emph{Gesetz der grossen Zahlen}).

Falls $P(|M_n - \mu| > \,\varepsilon) \le p$ sein soll:
\begin{align*}
  p & \le \frac{\var(X)}{n\,\varepsilon^2} \quad\Rightarrow\quad
  n \le \frac{\var(X)}{p\,\varepsilon^2} \\
  \text{worst case: } n &= \frac{\var(X)}{p\,\varepsilon^2} \quad {d.h.}\quad n \sim \frac{1}{\,\varepsilon^2}
\end{align*}

\subsection{Wie genau ist die Häufigkeit als Mass für die Wahrscheinlichkeit?}
Ereignis $A$ mit Wahrscheinlichkeit $P(A)$.

\begin{align*}
  h_n = \frac{\text{Anzahl eingetreten}}{n} & & P(|h_n - P(A)| > \,\varepsilon) =\text{ ?} \\% P(A) = E(\chi_A), \chi_A = X
h_n = \frac{X_1 + X_2 + \dots + X_n}{n} = M_n& & P(|M_n - \mu| > \,\varepsilon) \le \frac{\var(\chi_A)}{n\,\varepsilon^2} 
\end{align*}

\begin{align*}
\var(\chi_A) & = E(\chi_A^{\cancel{2}}) - E(\chi_A)^2 \\
& = P(A) - P(A)^2 \\
& = P(A) (1 - P(A)) \\
P(|h_n - P(A)| > \,\varepsilon) & \le \frac{P(A)(1-P(A))}{n\,\varepsilon^2} \le \frac{1}{4n\,\varepsilon^2} \qquad(\text{da } x(1-x) \le \frac{1}{4})
\end{align*}

\subsection{Anwendung: Lineare Regression}
Zwei Zufallsvariablen $X$, $Y$, mit $Y\approx aX+b$. Das Ziel ist, für Messungen $x_i, y_i$ bestmögliche $a$ und $b$ zu ermitteln.

\ifincludeExamples
\paragraph{Beispiel: Wie lange muss ein Verzögerungselement sein?}

\begin{itemize}
\item Konstante Abbrand-Geschwindigkeit, d.h. linearer Zusammenhang zwischen Länge und Brenndauer. $$L = aT + b$$
\end{itemize}

Gesucht ist nun $L = aT + b + Fehler$ mit möglichst kleinem Fehler.
\fi

\paragraph{Problem} Finde $a$, $b$ so, dass $Y-aX-b$ minimal.
\paragraph{Gesucht}
\begin{enumerate}
\item Im Mittel kein Fehler: $E(Y - aX-b) = E(Y) - aE(X) - b = 0$
\item Möglichst kleine Varianz: $var(Y - aX - b)$ minimal
\end{enumerate}

Das ist ein Optimierungsproblem. Idee: Ableiten.

\begin{align*}
  \var(Y - aX - b) &= E((Y-aX-b)^2) - E(Y-aX-b)^2 \\ 
                   &= E(Y^2) + a^2E(X^2) + \cancel{b^2} - 2aE(XY) - \cancel{2bE(Y)} + \cancel{2abE(X)} - \\
                   &  \quad \left(E(Y)^2 + a^2E(X)^2 + \cancel{b^2} - 2aE(X)E(Y) - \cancel{2bE(Y)} + \cancel{2abE(X)}\right) \\
                   &=  \var(Y) + a^2\var(X) - 2a\cov(X, Y) = Q\\[0.2cm]
  \frac{\partial Q}{\partial a} &= 2a\var(X) - 2\cov(X, Y) = 0 \\
                   a &= \frac{\cov(X, Y)}{\var(X)} \\
                   b &= E(Y) - aE(X)
\end{align*}

%Alternativer Weg: 
%\begin{align*}
%  Q & = E(Y^2) + a^2E(X^2) + b^2 - 2aE(XY) - 2bE(Y) + 2abE(X) \\
%  \frac{\partial Q}{\partial a} &= 2aE(X^2) - 2E(XY) + 2bE(X) = 0 \\
%  \frac{\partial Q}{\partial b} &= 2b - 2E(Y) + 2aE(X) = 0 \\
%  E(X^2)a + E(X)b & = E(XY) \\
%  E(X)a + b & = E(Y)
%\end{align*}
%\begin{align*}
%  a &= \begin{array}{c}\left[\begin{array}{cc}E(XY) & E(X) \\ E(Y) & 1\end{array}\right] \\ \hline  %FIXME
%                       \left[\begin{array}{cc}E(X^2) & E(X) \\ E(X) & 1\end{array}\right]\end{array} & \text{ Kramersche Regel}\\
%  & = \frac{E(XY) - E(X)E(Y)}{E(X^2)-E(X)^2} = \frac{\cov(X, Y)}{\var(X)}
%\end{align*}

Link zu Messdaten:

\begin{align*}
\text{Mittelwert: } \qquad E(X) & = \frac{1}{n}\sum_{i=1}^n x_i \\
                              a & = \frac{\frac{1}{n}\sum_{i=1}^n x_iy_i - \frac{1}{n^2}\sum_{i=1}^n x_i\sum_{i=1}^n y_i}{\frac{1}{n}\sum_{i=1}^n x_i^2 - \left(\frac{1}{n}\sum_{i=1}^n x_i\right)^2} \\
                              b & = E(Y) - aE(X) = \frac{1}{n}\sum_{i=1}^n x_i - a\frac{1}{n}\sum_{i=1}^n y_i
\end{align*}

Diese Formeln zeigen uns, dass es möglich ist, lineare Regression durchzuführen ohne grosse Speicher zu benötigen.

\subsection{Qualitätsmass für die Approximation}
Idee: $var(Y-aX-b)$ ``klein'' bedeutet gut, ``gross'' schlecht. Problem: Masseinheiten.

\begin{align*}
\var(Y-aX-b) & = \var(Y) + a^2\var(X) - 2a\cov(X, Y) & \text{ aber wir kennen jetzt } a\\
             & = \var(Y) + \frac{\cov(X, Y)^2}{\var(X)^{\cancel{2}}}\cancel{\var(X)} - 2\frac{\cov(X, Y)}{\var(X)} \cov(X,Y) \\
             & = \var(Y) - \frac{\cov(X, Y)^2}{\var(X)}\\
             & = \var(Y) \underbrace{\left(1 - \frac{\cov(X,Y)^2}{\var(X)\var(Y)}\right)}_{\text{dimensionslos}}
\end{align*}
Daraus können wir ableiten:
\begin{align*}
\text{``gut''} & \Leftrightarrow 1 - \frac{\cov(X,Y)^2}{\var(X)\var(Y)} \text{ klein} \\
               \Rightarrow \text{ Regressionskoeffizient } r^2 & = \frac{\cov(X,Y)^2}{\var(X)\var(Y)} \text{ nahe bei } 1 \\
\text{``schlecht''} & \Leftrightarrow r^2 = \frac{\cov(X,Y)^2}{\var(X)\var(Y)} \text{ nahe bei } 0 \text{ (Unabhängigkeit)} \\
r & := \frac{\cov(X,Y)}{\sqrt{\var(X)\var(Y)}}
\end{align*}
$r$ erlaubt uns, 3 Fälle zu unterscheiden:
\begin{description}
\item[$r\sim 0$] Punkte sind grösstenteils unabhängig voneinander (schlechtes Resultat)
\item[$r\sim 1$] Punkte sind abhängig voneinander (positive Korrelation), auf einer steigenden Geraden
\item[$r\sim -1$] Punkte sind abhängig voneinander (negative Korrelation), auf einer abfallenden Geraden
\end{description}

\section{Verteilungsfunktionen}
Die bisherige Vorgehensweise funktioniert gut für diskrete Zufallsvariablen mit wenigen Werten, stösst jedoch auf Probleme bei unendlich vielen, oder stetig verteilten (reelle Zahlen) Werten. Ausserdem mussten wir bisher genau wissen, wie $\Omega$ aussieht (dies erlaubte auch das Rechnen mit abhängigen Variablen).

\subsection{Definition}
Gesucht ist eine Funktion, welche alle relevante Wahrscheinlichkeitsinformation codiert:
$$X \leadsto F_X(x)$$

Ereignisse, die zu X gehören:
\begin{align*}
 \{X = a\} &  \leadsto P(X= a) = F_X(x) - \lim_{\varepsilon\to x-} F_X(\varepsilon) \\
 \{X\le a\} &  \leadsto P(X\le a) =: F_X(a) \\
 \{a < X \le b\} & \leadsto P(a < X \le b) = F(b) - F(a) 
\end{align*}

\ifincludeExamples
\paragraph{Beispiel} $X = $ ``Würfel''
$$ F_X(a) = \text{ Wahrscheinlichkeit, dass Augenzahl } \le a = P(x \le a) $$ % F_X(a) springt stufenweise 

Von der Verteilungsfunktion zur Wahrscheinlichkeit:
\begin{align*}
P(X=2) & = P(1.5 < X \le 2.5) = P(X\le2.5) - P(X\le1.5) = F(2.5) - F(1.5) \\
\text{besser } P(X=2) & = F(2) - \lim_{x\to2-} F(x)
\end{align*}

\paragraph{Beispiel: $\infty$ -Würfel}
Zylinder mit Endposition $X$ in $[0, 2\pi]$
\begin{align*}
P(a<X\le b) & = \frac{b-a}{2\pi} \\
F(a) = P(X\le a) & = P(0<X\le a) = \frac{a}{2\pi}
\end{align*}
\fi
\subsection{Eigenschaften von Verteilungsfunktionen}
\begin{align}
0 & \le F(x) \le 1 \\
x_1 \le x_2 & \Rightarrow F(x_1) \le F(x_2) & \text{ monoton steigend} \\
\lim_{x\to-\infty} F(x) = 0, & \quad\lim_{x\to\infty} F(x) = 1
\end{align}

\subsection{Erwartungswert}

Bisher haben wir den Erwartungswert wie folgt definiert:
$$
  E(X) = \sum_{\text{Werte}} \text{ Wert } \cdot \text{ Wahrscheinlichkeit }
$$

Problem: Unabzählbar viele Werte!

\begin{align*}
E(X) & = \sum_i x_i \cdot P(x_i < X \le x_{i+1}) \\
     & = \sum_i x_i \cdot \left(F(x_{i+1}) - F(x_i)\right) \\
     & = \sum_i x_i \cdot \left(F(x_{i+1}) - F(x_i)\right) \cdot \frac{x_{i+1} - x_i}{x_{i+1} - x_i} \\
     & = \sum_i x_i \cdot \underbrace{\frac{F(x_{i+1}) - F(x_i)}{x_{i+1} - x_i}}_{\text{mit} \Delta x \to 0 \text{: Ableitung}} x_{i+1} - x_i \\
\rightarrow  E(X) & = \int_{-\infty}^\infty x\cdot F'_X(x) dx = \int_{-\infty}^\infty x\cdot \varphi(x) dx \\
F_X'(x) & = \varphi_X (x)\text{ Wahrscheinlichkeit, Dichtefunktion} \\
F_X & = \text{ Stammfunktion von } \varphi_X = \int_{-\infty}^x \varphi_X(\xi)d\xi \\
P(a<X\le b) & = F_X(b) - F_X(a) = \int_a^b \varphi_X(x) dx
\end{align*}

\ifincludeExamples
\paragraph{Beispiel $\infty$-Würfel}
\begin{align*}
E(X) & = \int_{-\infty}^\infty x\cdot F'_X(x) dx & \text{F(x) ist } 0 \text{ links und rechts von } 0/2\pi \\
     & = \int_{-0}^{2\pi} x\cdot \frac{1}{2\pi} dx \\
     & = \left[ \frac{1}{4\pi} x^2\right]_0^{2\pi} = \frac{4\pi^2}{4\pi} = \pi 
\end{align*}
\fi

\subsection{Varianz}
\begin{align*}
\var(X) = E(X^2) - E(X)^2 \\
E(X^2) = \int_{-\infty}^\infty \underbrace{x^2}_{\text{Wert}}\qquad\cdot\underbrace{\varphi_X(x)}_{\text{Wahrscheinlichkeit}} dx \\
\end{align*}

\ifincludeExamples
\paragraph{Beispiel $\infty$-Würfel}
\begin{align*}
E(X^2) & = \int_{-\infty}^\infty x^2 \varphi_X(x) dx = \int_{-\infty}^\infty x^2 \frac{1}{2\pi} dx 
         = \left[\frac{x^3}{6\pi}\right]_0^{2\pi} = \frac{8\pi^3}{6\pi} = \frac{4}{3}\pi^2 \\
\var(X) & = E(X^2) - E(X)^2 = \frac{4}{3}\pi^2 - \pi^2 = \frac{1}{3}\pi^2, stddev = \frac{1}{\sqrt{3}}\pi = 0.58\pi
\end{align*}
\fi

\subsection{Eigenschaften der Dichtefunktion von $\varphi_X$ } % folgen aus Eigenschaften von F_X
\begin{align}
F_X \text{ monoton wachsend } &\Rightarrow \quad\varphi_X(x) = F_X'(x) \ge 0 \\
F_X(x) \to 1 \text{ für } x\to \infty &\Rightarrow \int_{-\infty}^\infty \varphi_X(x) dx 
  = \lim_{a\to\infty} \underbrace{\int_{-\infty}^a\varphi_X(x) dx}_{F_X(a)} = 1
\end{align}

\ifincludeExamples
\begin{align*}
\varphi(x) = \begin{cases}a\cos x & -\frac{\pi}{2} \le x \frac{\pi}{2} \\ 0 & \text{ sonst}\end{cases} \\
\varphi(x) \ge 0 & \forall x\in\mathbb{R} \text{ ok} \\
\int_{-\infty}^\infty \varphi(x) dx = \int_{-\frac{\pi}{2}}^\frac{\pi}{2} a\cos x dx = a\left[sin x\right]_{-\frac{\pi}{2}}^\frac{\pi}{2} = 2a
\end{align*}
Also eine gültige Dichtefunktion für $a=\frac{1}{2}$
\begin{align*}
F(X) = \int_{-\infty}^x \varphi(\xi)d\xi = \int_{-\frac{\pi}{2}}^x \frac{1}{2}\cos\xi d\xi = \frac{1}{2}\left(sin(x) + 1\right)
\end{align*}
\fi

\subsection{Berechnung von Varianten} % TODO: fix title
Algorithmus am Beispiel von $F_{X^2}$, $\varphi_{X^2}$:
\begin{align*}
F_{X^2} & = P(X^2 \le x) & \text{Definition verwenden}\\
        & = P(|X| \le \sqrt{x}) & \text{Umformen auf } P(X \le \dots) \\
        & = P(-\sqrt{x} \le X \le \sqrt{x}) \\
        & = F_X(\sqrt{x}) - F_X(-\sqrt{x}) & \text{Definition von } F_X\text{ anwenden} \\
\varphi_{X^2} & = \frac{d}{dX} F_{X^2}(x) = \frac{d}{dX}\left(F_X(\sqrt{x}) - F_X(-\sqrt{x}\right) \\
        & = F'_X(\sqrt{x}) \frac{1}{2\sqrt{x}} + F'_X(-\sqrt{x}) \frac{1}{2\sqrt{x}} \\
        & = \frac{1}{2\sqrt{x}} \left( F'_X(\sqrt{x}) + F'_X(-\sqrt{x})\right) \\
        & = \frac{1}{2\sqrt{x}} \left(\varphi_X(\sqrt{x}) + \varphi_X(-\sqrt{x})\right) \\[1cm]
F_{\sqrt{X}} & = P(\sqrt{X} \le x) \stackrel{X\ge0}{=} P(X \le x^2) \\
             & = F_X(x^2) \\
\varphi_{\sqrt{X}} & = \frac{d}{dx} F_{\sqrt{X}}(x) = \frac{d}{dx} F_X(x^2) = 2x \cdot F_X(x^2) = 2x\cdot\varphi_X(x^2)
\end{align*}
\begin{align*}
F_{\lambda X}(x) & = P(\lambda X \le x) = P\left(X \le \frac{x}{\lambda}\right) & \lambda > 0 \\
                 & = F_X\left(\frac{x}{\lambda}\right) \\
\varphi_{\lambda X}(x) & = \frac{d}{dx}F_{\lambda X}(x) = \frac{d}{dx}F_{X}\left(\frac{x}{\lambda}\right) \\
                       & = F'_X\left(\frac{x}{\lambda}\right) \frac{1}{\lambda} = \varphi_X\left(\frac{x}{\lambda}\right)\frac{1}{\lambda} \\
E(\lambda X) & = \int_{-\infty}^\infty x\varphi_{\lambda X} (x)dx \\
             & = \lambda\int_{-\infty}^\infty \underbrace{\frac{x}{\lambda}}_z \varphi_X(\underbrace{\frac{x}{\lambda}}_z) \underbrace{\frac{dx}{\lambda}}_{dz} \\
             & = \lambda \int_{-\infty}^\infty z\varphi_X(z) dz = \lambda E(X)
\end{align*}

Annahme: $X$, $Y$ unabhängig.
\begin{align*}
\varphi_{X+Y}(s) \cdot \Delta s & = \text{ Wahrscheinlichkeit, dass } X+Y \text{ in einem Interval } \Delta s \text{ um } s \text{ liegt} \\
 & = \underbrace{\text{ Wahrscheinlichkeit, dass } X \text{ in einem Interval um } x \text{ liegt }}_{\varphi_X(x) \cdot \text{ Interval}} \wedge \\& \qquad\underbrace{\text{ dass } Y \text{ in einem Interval um } s-x}_{\varphi_Y(s-x)\cdot \text{ Interval}} \underbrace{\text{ für alle } x-\text{ Intervalle liegt}}_{\rightarrow Summe} \\
  &= \int_{-\infty}^\infty \varphi_X(x) \varphi_Y(s-x) dx \Delta s \\
  \Rightarrow \varphi_{X+Y}(s) & = \int_{-\infty}^\infty \varphi_X(x) \varphi_Y(s-x) dx = \varphi_X *\ \varphi_Y(s) & = \text{ Faltung}
\end{align*}

\ifincludeExamples
Beispiel mit $X$, $Y$ gleichverteilt in [0,1]:
\begin{align*}
\varphi_X(x) & = \begin{cases} 0 & x \le 0 \\ 1 & 0 < x \le 1 \\ 0 & x > 1\end{cases}\\
\varphi_{Y}(s-x) & = \begin{cases}0 & s-x> 1 \Leftrightarrow x < s-1 \\ 1 & s-1\le x < s \\ 0 & x\ge s\end{cases} \\
\varphi_{X+Y} (s) & = \int_{-\infty}^\infty \varphi_X(x)\varphi_Y(s-x) dx & \varphi_X(x) \text{ ist immer 1 im relevanten Bereich}\\
                  & = \int_0^1 \varphi_Y(s-x) dx \\
\varphi_{X+Y}(s) & = \begin{cases} 0 & s < 0 \\ s & 0\le s <1 \\1-(s-1) = 2-s & 1 \le s \le 2\\ 0 & s > 2 \end{cases} \\
\end{align*}
\fi

\paragraph{Erzeugung von Zufallszahlen mit Verteilungsfunktion $F$}
\begin{description}
\item[Gegeben] Zufallsvariable $X$ mit Verteilungsfunktion $F$, Y = F(X), Y ist ZV mit Werten in [0, 1].
\item[Gesucht] $F_Y$
\end{description}

\begin{align*}
  F_Y(y) & \stackrel{\text{Def}}{=} P(Y \le y) \\
         & = P(F(X) \le y)  & F \text{ umkehrbar da monoton wachsend} \\
         & = P(X \le F^{-1}(y)) \\
         & \stackrel{\text{Def}}{=} F(F^{-1}(y)) = y 
\end{align*}

$F(X)$ ist also gleichverteilt!
Umgekehrt: $Y$ gleichverteilt $\Rightarrow F^{-1}(Y)$ hat Verteilungsfunktion $F$.

\section{Katalog von Wahrscheinlichkeitsverteilungen}
\newlength{\katalogSpaltezwei}
\setlength{\katalogSpaltezwei}{11.5cm}

\subsection{Gleichverteilung in Intervall $[\,a, b\,]$}
\begin{tabular}{r p{\katalogSpaltezwei}}
Anwendung & Zufallszahlen \\
Dichtefunktion & $\varphi(x) = \begin{cases}\frac{1}{b-a} & a < x < b \\ 0 & \text{sonst} \end{cases} $ \\
Verteilungsfunktion & $F(x) = \begin{cases}0 & x < a \\ \frac{x-a}{b-a} & a < x < b \\ 1 & x > b\end{cases} $ \\
Erwartungswert & $E(X) = \frac{a+b}{2}$ \\
Varianz & $\var(X) = \frac{(b-a)^2}{12}$ \\
Median & $ \median(X) = \frac{a+b}{2} $
\end{tabular}


\subsection{Exponentialverteilung}
\begin{tabular}{r p{\katalogSpaltezwei}}
Anwendung & Zeitabhängige Prozesse ohne Erinnerungsvermögen (ungesättigte Queues, 
            Ausfall von Hardware ohne Alterungsprozess, radioaktiver Zerfall) \\
Dichtefunktion & $\varphi(x) = \begin{cases} 0 & x < 0 \\ ae^{-ax} & x \ge 0 \end{cases} $ \\
Verteilungsfunktion & $F(x) = \begin{cases} 0 & x<0 \\ 1 - e^{-ax} & x \ge 0 \end{cases} $ \\
Erwartungswert & $E(X) = $ \\
Varianz & $\var(X) = $ \\
Median & $ \median(X) =  $ 
\end{tabular} 

\subsubsection{Herleitung}
\begin{align*}
\text{kein Erinnerungsvermögen}: P(X\le t) & = P(X \le t + t_0 | X > t_0) \\ 
1 - F(t) & = P(X>t) = g(t) \\
P(X>t) & = P(X > t+t_0 | X > t_0) & \text{mit } P(A|B) = \frac{P(A\cap B)}{P(B)}\\ 
 & =  \frac{P(X > t+t_0 \wedge \cancel{X > t_0})}{P(X>t_0)} \\ 
 & = \frac{P(X > t+t_0)}{P(X>t_0)} \\
 & = \frac{g(t+t_0)}{g(t_0)} \\
 \text{Funktionalgleichungen } g(t+t_0) & = g(t)g(t_0) & \text{``Lösung durch Anstarren'': } e^x\\
\end{align*}
Wir dürfen aus der Konstruktion annehmen, dass $g'$ existiert.
\begin{align*}
\text{Ableiten nach} t_0: g'(t+t_0) & = g(t)g't(t_0) \\
\lim_{t_0 \to 0} g'(t) & = g(t)g'(0) & \text{DGL für } g \\
\Rightarrow g(t) & = e^{-at} 
\end{align*}

\end{document}